\documentclass[12pt,conference]{ieeeconf}
\bibliographystyle{ieeetr}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{float}
\graphicspath{{/}}
\begin{document}
\title{Lensless Image Classification using Deep Learning}
\author{Ganghun Kim\textsuperscript{1}, Brian Rodriguez\textsuperscript{1}, Rajesh Menon\textsuperscript{*}}
\maketitle

\abstract
Deep Learning (DL) has accelerated advancements in Image Classification via convolutional neural networks (CNNs). However, these image classification tasks have been widely trained on images taken with typical cameras; human consumable photographs. Here, we present a CNN trained using data taken by a single CMOS image sensor with no lens. We created a dataset of lensless images comprised of handwritten digits taken from the MNIST dataset. Then, we trained a CNN on this dataset and we're able to show that for 10 digits, the CNN is able to classify lensless images with a 96.6\% accuracy.

\section{Introduction}
Wide-scale deep learning algorithms have pushed Image Classification to its limits. State-of-the-art architectures have been able to classify human consumable images with astonishing accuracy. 

Recently, there have been advances in the space of lensless imaging, where a single CMOS image sensor is utilized to take an image without a lens.

\section{Background}

\section{Proposal}
We propose a novel CNN architecture that is able to classify lensless images at a 96.6\% accuracy. Our architecture is comprised of four sections containing convolutional layers and maxpooling layers, for downsampling. Following these four sections is a classifying section that contains two fully-connected layers, we also employed dropout with a probability of .5 to prevent overfitting in the network (ref). Each convolutional layer is followed by batch normalization and a ReLU activation function, except for the 1x1 convolutional layers which are used for dimensionality reduction after every pooling layer, excluding the final pooling layer.

\section{Training Methodology}
We created our network using Pytorch, a highly extensible deep learning framework. Our network was trained using stochastic gradient descent (SGD) with a momentum of .9 on a single NVidia Tesla V100 GPU (ref). While other gradient optimizers were tested such as Adam and Adagrad, they did not converge accordingly (ref). 

\section{Results}

\section{Related Work}

\section{Conclusion}

\bibliography{biblio}

\end{document}